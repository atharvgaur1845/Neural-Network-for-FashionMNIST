{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "KlQ_bgOhzl0l"
      },
      "outputs": [],
      "source": [
        "import os, numpy as np, matplotlib.pyplot as plt\n",
        "folder = \"/content/final\" # folder containing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "mpZT6bjQ0GiY"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(\"/content/sample_data/fashion-minst.zip\", \"r\") as zip_ref:\n",
        "     zip_ref.extractall(\".\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b84yACg0Iwx",
        "outputId": "294b10e8-1f75-4579-d00b-edd99675bc93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 1 class\n",
            "Loaded 9 class\n",
            "Loaded 3 class\n",
            "Loaded 4 class\n",
            "Loaded 0 class\n",
            "Loaded 7 class\n",
            "Loaded 6 class\n",
            "Loaded 5 class\n",
            "Loaded 2 class\n",
            "Loaded 8 class\n",
            "Before reshaping\n",
            "(60000, 28, 28, 4) (60000, 10)\n",
            "[[[0. 0. 0. 1.]\n",
            "  [0. 0. 0. 1.]\n",
            "  [0. 0. 0. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. 1.]\n",
            "  [0. 0. 0. 1.]\n",
            "  [0. 0. 0. 1.]]\n",
            "\n",
            " [[0. 0. 0. 1.]\n",
            "  [0. 0. 0. 1.]\n",
            "  [0. 0. 0. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. 1.]\n",
            "  [0. 0. 0. 1.]\n",
            "  [0. 0. 0. 1.]]\n",
            "\n",
            " [[0. 0. 0. 1.]\n",
            "  [0. 0. 0. 1.]\n",
            "  [0. 0. 0. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. 1.]\n",
            "  [0. 0. 0. 1.]\n",
            "  [0. 0. 0. 1.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0. 0. 0. 1.]\n",
            "  [0. 0. 0. 1.]\n",
            "  [0. 0. 0. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. 1.]\n",
            "  [0. 0. 0. 1.]\n",
            "  [0. 0. 0. 1.]]\n",
            "\n",
            " [[0. 0. 0. 1.]\n",
            "  [0. 0. 0. 1.]\n",
            "  [0. 0. 0. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. 1.]\n",
            "  [0. 0. 0. 1.]\n",
            "  [0. 0. 0. 1.]]\n",
            "\n",
            " [[0. 0. 0. 1.]\n",
            "  [0. 0. 0. 1.]\n",
            "  [0. 0. 0. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. 1.]\n",
            "  [0. 0. 0. 1.]\n",
            "  [0. 0. 0. 1.]]] [0 1 0 0 0 0 0 0 0 0]\n",
            "After reshaping\n",
            "(60000, 784) (60000, 10)\n",
            "[0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.40784314 0.45490196\n",
            " 0.44313726 0.6745098  0.6392157  0.6392157  0.5647059  0.53333336\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.06666667 0.5764706  0.5411765  0.8        1.\n",
            " 0.61960787 0.7411765  0.6313726  0.7764706  0.2        0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.14117648\n",
            " 0.52156866 0.50980395 0.69803923 0.7529412  0.49803922 0.654902\n",
            " 0.5647059  0.6862745  0.3764706  0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.23137255 0.4745098  0.53333336\n",
            " 0.6666667  0.70980394 0.5411765  0.5882353  0.5882353  0.70980394\n",
            " 0.4        0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.24313726 0.46666667 0.46666667 0.61960787 0.6392157\n",
            " 0.52156866 0.5411765  0.53333336 0.6666667  0.41960785 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.2\n",
            " 0.60784316 0.6666667  0.5882353  0.5529412  0.4862745  0.5882353\n",
            " 0.53333336 0.6666667  0.41960785 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.24313726 0.53333336 0.5647059\n",
            " 0.6862745  0.7764706  0.44313726 0.6        0.60784316 0.6666667\n",
            " 0.41960785 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.21960784 0.4862745  0.50980395 0.7529412  0.61960787\n",
            " 0.34117648 0.5882353  0.6        0.6862745  0.3882353  0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.2\n",
            " 0.50980395 0.5647059  0.7529412  0.46666667 0.26666668 0.5529412\n",
            " 0.52156866 0.6862745  0.31764707 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.09803922 0.53333336 0.5411765\n",
            " 0.7529412  0.45490196 0.14117648 0.60784316 0.50980395 0.6745098\n",
            " 0.2        0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.00784314 0.49803922 0.5411765  0.8        0.44313726\n",
            " 0.06666667 0.60784316 0.4862745  0.6666667  0.13333334 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.4745098  0.5411765  0.7882353  0.34901962 0.         0.60784316\n",
            " 0.50980395 0.6862745  0.12156863 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.46666667 0.53333336\n",
            " 0.7764706  0.25490198 0.         0.60784316 0.53333336 0.69803923\n",
            " 0.07450981 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.46666667 0.53333336 0.70980394 0.14117648\n",
            " 0.         0.5764706  0.5411765  0.6313726  0.04313726 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.44313726 0.5411765  0.6745098  0.01960784 0.         0.5411765\n",
            " 0.5529412  0.53333336 0.08627451 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.33333334 0.50980395\n",
            " 0.654902   0.         0.         0.46666667 0.5647059  0.6\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.28627452 0.46666667 0.6313726  0.\n",
            " 0.         0.3882353  0.5647059  0.49803922 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.27450982 0.43137255 0.60784316 0.         0.         0.34901962\n",
            " 0.5764706  0.53333336 0.06666667 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.3647059  0.4745098\n",
            " 0.50980395 0.         0.         0.23137255 0.5764706  0.53333336\n",
            " 0.03137255 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.28627452 0.5882353  0.60784316 0.00784314\n",
            " 0.         0.25490198 0.5411765  0.5529412  0.01960784 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.27450982 0.6313726  0.6862745  0.04313726 0.         0.4\n",
            " 0.5529412  0.53333336 0.03137255 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.25490198 0.6\n",
            " 0.654902   0.03137255 0.         0.3647059  0.5411765  0.5411765\n",
            " 0.05490196 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.17254902 0.5882353  0.70980394 0.08627451\n",
            " 0.         0.3882353  0.5647059  0.53333336 0.03137255 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.17254902 0.6392157  0.7529412  0.08627451 0.         0.45490196\n",
            " 0.5529412  0.53333336 0.04313726 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.1882353  0.53333336\n",
            " 0.69803923 0.09803922 0.         0.41960785 0.61960787 0.5529412\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.13333334 0.52156866 0.7647059  0.24313726\n",
            " 0.         0.3882353  0.6666667  0.40784314 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.53333336 0.7764706  0.21960784 0.         0.34117648\n",
            " 0.72156864 0.33333334 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.28627452\n",
            " 0.6        0.         0.         0.21960784 0.654902   0.15294118\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.        ] [0 1 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def load_data(X, y):\n",
        "    for f in os.listdir(folder):\n",
        "        for file in os.listdir(f\"{folder}/{f}\"):\n",
        "            img = plt.imread(f\"{folder}/{f}/{file}\")\n",
        "            X.append(img)\n",
        "\n",
        "           \n",
        "\n",
        "            label = [0] * 10\n",
        "            label[int(f)] = 1 \n",
        "            y.append(label)\n",
        "\n",
        "        print(f\"Loaded {f} class\")\n",
        "\n",
        "X, y = [], []\n",
        "load_data(X, y)\n",
        "\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "print(\"Before reshaping\")\n",
        "print(X.shape, y.shape)\n",
        "print(X[0], y[0])\n",
        "\n",
        "X = X[:, :,:, 0] \n",
        "X = X.reshape(X.shape[0], X.shape[1]*X.shape[2])\n",
        "print(\"After reshaping\")\n",
        "print(X.shape, y.shape)\n",
        "print(X[0], y[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Kt00ajR00Nh_"
      },
      "outputs": [],
      "source": [
        "class NN:\n",
        "    def __init__(self, input_neurons, hidden_neurons, output_neurons, learning_rate, epochs):\n",
        "        \"\"\"\n",
        "        Class Definition\n",
        "\n",
        "        We use a class because it is easy to visualize the process of training a neural network\n",
        "        It's also easier to resuse and repurpose depending on the task at hand\n",
        "\n",
        "        We have a simple neural network, with an input layer, one hidden (middle) layer and an output layer\n",
        "\n",
        "        input_neurons: Number of neurons in the input layer\n",
        "        hidden_neurons: Number of neurons in the hidden layer\n",
        "        output_neurons: Number of neurons in the output layer\n",
        "        learning_rate: The rate at which the weights are updated [Q5] What is the learning rate? answer - it is given as how fast or slow the model learns.\n",
        "        epochs: Number of times the model will train on the entire dataset\n",
        "        \"\"\"\n",
        "\n",
        "        self.input_neurons = input_neurons\n",
        "        self.hidden_neurons = hidden_neurons\n",
        "        self.output_neurons = output_neurons\n",
        "        self.epochs = epochs\n",
        "\n",
        "        self.lr = learning_rate\n",
        "\n",
        "        \n",
        "\n",
        "        self.wih = np.random.randn(hidden_neurons, input_neurons) * np.sqrt(2/input_neurons)\n",
        "        self.bih = np.zeros((hidden_neurons, 1))\n",
        "\n",
        "        self.who = np.random.randn(output_neurons, hidden_neurons) * np.sqrt(2/hidden_neurons)\n",
        "        self.bho = np.zeros((output_neurons, 1))\n",
        "\n",
        "    \n",
        "\n",
        "    def relu(self, z):\n",
        "        \"\"\"\n",
        "        Implementation of the RELU function\n",
        "        z: (n, 1)\n",
        "        returns (n, 1)\n",
        "        \"\"\"\n",
        "        return z * (z > 0)\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"\n",
        "        Implementation of the Sigmoid function\n",
        "        z: (n, 1)\n",
        "        returns (n, 1)\n",
        "        \"\"\"\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def relu_derivative(self, z):\n",
        "        \"\"\"\n",
        "        Implementation of the RELU derivative function\n",
        "        z: (n, 1)\n",
        "        returns (n, 1)\n",
        "        \"\"\"\n",
        "        return 1 * (z > 0)\n",
        "\n",
        "    def sigmoid_derivative(self, z):\n",
        "        \"\"\"\n",
        "        Implementation of the Sigmoid derivative function\n",
        "        z: (n, 1)\n",
        "        returns (n, 1)\n",
        "        \"\"\"\n",
        "        return z * (1 - z)\n",
        "\n",
        "   \n",
        "    def softmax(self, z):\n",
        "        \"\"\"\n",
        "        Implementation of the Softmax function\n",
        "        z: (n, 1)\n",
        "        returns (n, 1)\n",
        "        \"\"\"\n",
        "        exp_z = np.exp(z - np.max(z))\n",
        "        return np.exp(z) / np.sum(np.exp(z), axis=0)\n",
        "\n",
        "    def softmax_derivative(self, z):\n",
        "        \"\"\"\n",
        "        Implementation of the Softmax derivative function\n",
        "        z: (n, 1)\n",
        "        returns (n, 1)\n",
        "        \"\"\"\n",
        "        return z * (1 - z)\n",
        "\n",
        "    \n",
        "\n",
        "    def mean_squared_error(self, y, y_hat):\n",
        "        \"\"\"\n",
        "        Implementation of the Mean Squared Error function\n",
        "        y: (10, n)\n",
        "        y_hat: (10, n)\n",
        "        returns (1, n)\n",
        "        \"\"\"\n",
        "        return np.mean((y - y_hat) ** 2, axis=0)\n",
        "\n",
        "    def cross_entropy_loss(self, y, y_hat):\n",
        "        \"\"\"\n",
        "        Implementation of the Cross Entropy Loss function\n",
        "        y: (10, n)\n",
        "        y_hat: (10, n)\n",
        "        returns (1, n)\n",
        "        \"\"\"\n",
        "\n",
        "        \n",
        "        epsilon = 1e-15\n",
        "        y_hat = np.clip(y_hat, epsilon, 1 - epsilon)\n",
        "\n",
        "        # Calculate the cross-entropy loss\n",
        "        loss = -np.sum(y * np.log(y_hat), axis=0)\n",
        "\n",
        "        return loss.reshape(y.shape[0], -1)\n",
        "\n",
        "        pass\n",
        "\n",
        "    def mean_squared_error_derivative(self, y, y_hat):\n",
        "        \"\"\"\n",
        "        Implementation of the Mean Squared Error derivative function\n",
        "        y: (10, n)\n",
        "        y_hat: (10, n)\n",
        "        returns (10, n)\n",
        "        \"\"\"\n",
        "        return y_hat - y\n",
        "\n",
        "    def cross_entropy_derivative(self, y, y_hat):\n",
        "        \"\"\"\n",
        "        Implementation of the Cross Entropy Loss derivative function\n",
        "        y: (10, n)\n",
        "        y_hat: (10, n)\n",
        "        returns (10, n)\n",
        "        \"\"\"\n",
        "\n",
        "        # Implement the cross entropy loss derivative function here and return it\n",
        "        # Note: The derivative of the CEL is usually taken with respect to the softmax input not output so keep that in mind while writing\n",
        "        return y_hat - y\n",
        "\n",
        "        pass\n",
        "\n",
        "    # Forward propagation\n",
        "    def forward(self, input_list):\n",
        "        \"\"\"\n",
        "        Implementation of the Forward Pass\n",
        "        input_list: (784, n)        - n is the number of images\n",
        "        returns (10, n)              - n is the number of images\n",
        "\n",
        "        Now we come to the heart of the neural network, the forward pass\n",
        "        This is where the input is passed through the network to get the output\n",
        "\n",
        "        [Q12] What does the output choice we have here mean? It's an array of 10 elements per image, but why?\n",
        "        it represents the probablity of the image in each class\n",
        "        \"\"\"\n",
        "\n",
        "        inputs = np.array(input_list, ndmin=2).T\n",
        "        inputs = inputs - np.mean(inputs) \n",
        "\n",
        "        # To get to the hidden layer:\n",
        "        # Multiply the input with the weights and adding the bias\n",
        "        # Apply the activation function (relu in this case)\n",
        "\n",
        "     \n",
        "        hidden_layer_input = np.dot(self.wih, inputs) + self.bih\n",
        "        hidden_layer_output = self.relu(hidden_layer_input)\n",
        "\n",
        "\n",
        "        \n",
        "        output_layer_input = np.dot(self.who, hidden_layer_output) + self.bho\n",
        "        output_layer_output = self.softmax(output_layer_input)\n",
        "\n",
        "\n",
        "        # Return it\n",
        "\n",
        "        \n",
        "        return output_layer_output\n",
        "\n",
        "\n",
        "    # Back propagation\n",
        "    def backprop(self, inputs_list, targets_list):\n",
        "        \"\"\"\n",
        "        Implementation of the Backward Pass\n",
        "        inputs_list: (784, n)\n",
        "        targets_list: (10, n)\n",
        "        returns a scalar value (loss)\n",
        "        \"\"\"\n",
        "        # Basic forward pass to get the outputs\n",
        "        inputs = np.array(inputs_list, ndmin=2).T  # (784, n)\n",
        "        inputs = inputs - np.mean(inputs)\n",
        "\n",
        "        # Reshape target to match the output of softmax function shape: (output_neurons, num_samples)\n",
        "        targets = np.array(targets_list, ndmin=2).T  # (10, n)\n",
        "\n",
        "        hidden_inputs = np.dot(self.wih, inputs) + self.bih\n",
        "        hidden_outputs = self.relu(hidden_inputs)\n",
        "\n",
        "        final_inputs = np.dot(self.who, hidden_outputs) + self.bho\n",
        "        outputs = self.softmax(final_inputs)  # Outputs from softmax function\n",
        "\n",
        "        # Calculating the loss using cross-entropy\n",
        "        loss = self.cross_entropy_loss(targets, outputs)\n",
        "\n",
        "        # Updating the weights using Update Rule for Cross Entropy Loss\n",
        "        dE_dzo = self.cross_entropy_derivative(targets, outputs) # (10, n)\n",
        "\n",
        "        # Update output layer weights and bias\n",
        "        dE_dwho = np.dot(dE_dzo, hidden_outputs.T) / inputs.shape[1] # (10, 160) - Dividing by number of samples for averaging the gradient\n",
        "        dE_dbho = np.mean(dE_dzo, axis=1, keepdims=True)  # (10, 1)\n",
        "\n",
        "        self.who -= self.lr * dE_dwho\n",
        "        self.bho -= self.lr * dE_dbho\n",
        "\n",
        "        # Update hidden layer weights and bias\n",
        "        dE_dah = np.dot(self.who.T, dE_dzo)  # (160, n)\n",
        "        dE_dzh = dE_dah * self.relu_derivative(hidden_inputs) # (160, n)\n",
        "        dE_dwih = np.dot(dE_dzh, inputs.T) / inputs.shape[1] # (160, 784) - Dividing by number of samples for averaging the gradient\n",
        "        dE_dbih = np.mean(dE_dzh, axis=1, keepdims=True)  # (160, 1)\n",
        "\n",
        "        self.wih -= self.lr * dE_dwih\n",
        "        self.bih -= self.lr * dE_dbih\n",
        "\n",
        "        return np.mean(loss)\n",
        "\n",
        "    def fit(self, inputs_list, targets_list,validation_data, validation_labels):\n",
        "       \n",
        "\n",
        "        train_loss = []\n",
        "        val_loss = []\n",
        "        for epoch in range(self.epochs):\n",
        "            loss = self.backprop(inputs_list, targets_list)\n",
        "            train_loss.append(loss)\n",
        "            vloss = self.cross_entropy_loss(validation_labels.T, self.forward(validation_data))\n",
        "            val_loss.append(np.mean(vloss))\n",
        "            print(f\"Epoch: {epoch}, Loss: {loss}, Val Loss: {val_loss[-1]}\")\n",
        "\n",
        "        return train_loss[1:], val_loss[:-1]\n",
        "\n",
        "    def predict(self, X):\n",
        "        outputs = self.forward(X).T\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "w_hqrZ3g0X9i",
        "outputId": "c03890a0-0677-4821-8f60-b78f53bde12c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss: 2.4996656762465856, Val Loss: 2.39235638954744\n",
            "Epoch: 1, Loss: 2.3926013776217356, Val Loss: 2.302714355069817\n",
            "Epoch: 2, Loss: 2.300970281965625, Val Loss: 2.22416111444831\n",
            "Epoch: 3, Loss: 2.2206933384575365, Val Loss: 2.154314929384754\n",
            "Epoch: 4, Loss: 2.149351932992558, Val Loss: 2.091587083078068\n",
            "Epoch: 5, Loss: 2.085269019924669, Val Loss: 2.0347200291423846\n",
            "Epoch: 6, Loss: 2.027225654526764, Val Loss: 1.9828092018456187\n",
            "Epoch: 7, Loss: 1.974298033663404, Val Loss: 1.935091513910531\n",
            "Epoch: 8, Loss: 1.9256709540177368, Val Loss: 1.8909203474856506\n",
            "Epoch: 9, Loss: 1.8807126775667518, Val Loss: 1.849781166715122\n",
            "Epoch: 10, Loss: 1.83888611269571, Val Loss: 1.811270735383357\n",
            "Epoch: 11, Loss: 1.799763345365745, Val Loss: 1.7750526468246592\n",
            "Epoch: 12, Loss: 1.762996200269141, Val Loss: 1.7408730230572855\n",
            "Epoch: 13, Loss: 1.7283187665244606, Val Loss: 1.7085002470971438\n",
            "Epoch: 14, Loss: 1.6954828319459896, Val Loss: 1.6777484579661353\n",
            "Epoch: 15, Loss: 1.6643153533644235, Val Loss: 1.648473092639562\n",
            "Epoch: 16, Loss: 1.6346587785780982, Val Loss: 1.6205636655836118\n",
            "Epoch: 17, Loss: 1.6063868466696953, Val Loss: 1.5939161925991991\n",
            "Epoch: 18, Loss: 1.5793948503458035, Val Loss: 1.5684170569232632\n",
            "Epoch: 19, Loss: 1.5535836845144164, Val Loss: 1.544007326675694\n",
            "Epoch: 20, Loss: 1.528883607155288, Val Loss: 1.5206248090017802\n",
            "Epoch: 21, Loss: 1.5052343772947252, Val Loss: 1.4982174798104828\n",
            "Epoch: 22, Loss: 1.4825681746383197, Val Loss: 1.4767280115393595\n",
            "Epoch: 23, Loss: 1.46082247910096, Val Loss: 1.456108594868682\n",
            "Epoch: 24, Loss: 1.4399453500894397, Val Loss: 1.4363162870262136\n",
            "Epoch: 25, Loss: 1.4199014853893739, Val Loss: 1.417303569475572\n",
            "Epoch: 26, Loss: 1.4006465414196714, Val Loss: 1.3990431367800102\n",
            "Epoch: 27, Loss: 1.3821382189569744, Val Loss: 1.3814912265196349\n",
            "Epoch: 28, Loss: 1.364338906073043, Val Loss: 1.3646210099382705\n",
            "Epoch: 29, Loss: 1.3472248627993377, Val Loss: 1.3484036644154178\n",
            "Epoch: 30, Loss: 1.3307648233848648, Val Loss: 1.3328027891958376\n",
            "Epoch: 31, Loss: 1.3149316670544697, Val Loss: 1.3177896362421249\n",
            "Epoch: 32, Loss: 1.2996990956011862, Val Loss: 1.303349865042687\n",
            "Epoch: 33, Loss: 1.2850418220451874, Val Loss: 1.2894507686754035\n",
            "Epoch: 34, Loss: 1.270927449784638, Val Loss: 1.276066564953251\n",
            "Epoch: 35, Loss: 1.2573356305468557, Val Loss: 1.263171261249562\n",
            "Epoch: 36, Loss: 1.2442393866022843, Val Loss: 1.2507433913665915\n",
            "Epoch: 37, Loss: 1.2316139922717584, Val Loss: 1.2387623809266088\n",
            "Epoch: 38, Loss: 1.219434890815219, Val Loss: 1.2271991304270298\n",
            "Epoch: 39, Loss: 1.2076781750861896, Val Loss: 1.2160350365209627\n",
            "Epoch: 40, Loss: 1.1963278739188987, Val Loss: 1.2052514113916297\n",
            "Epoch: 41, Loss: 1.1853638908037363, Val Loss: 1.194828031514944\n",
            "Epoch: 42, Loss: 1.1747703742864195, Val Loss: 1.1847478843943793\n",
            "Epoch: 43, Loss: 1.1645275864579152, Val Loss: 1.1749945402887552\n",
            "Epoch: 44, Loss: 1.1546156653874564, Val Loss: 1.1655525093769177\n",
            "Epoch: 45, Loss: 1.1450166935711015, Val Loss: 1.1564078578992203\n",
            "Epoch: 46, Loss: 1.1357186327798159, Val Loss: 1.147547266262329\n",
            "Epoch: 47, Loss: 1.126708106287304, Val Loss: 1.1389573584804134\n",
            "Epoch: 48, Loss: 1.1179685776490056, Val Loss: 1.130617115076971\n",
            "Epoch: 49, Loss: 1.1094835243583967, Val Loss: 1.1225184838867368\n",
            "Epoch: 50, Loss: 1.1012459644302124, Val Loss: 1.114652793528388\n",
            "Epoch: 51, Loss: 1.0932448368012548, Val Loss: 1.1070148679817884\n",
            "Epoch: 52, Loss: 1.0854744265202823, Val Loss: 1.0995883389070813\n",
            "Epoch: 53, Loss: 1.0779174862694767, Val Loss: 1.092364893113462\n",
            "Epoch: 54, Loss: 1.0705638805530704, Val Loss: 1.0853323484540958\n",
            "Epoch: 55, Loss: 1.0634043974264316, Val Loss: 1.0784893431014877\n",
            "Epoch: 56, Loss: 1.0564379327463236, Val Loss: 1.0718274641809484\n",
            "Epoch: 57, Loss: 1.049653117500598, Val Loss: 1.0653351041348098\n",
            "Epoch: 58, Loss: 1.0430408400284192, Val Loss: 1.0590078690003748\n",
            "Epoch: 59, Loss: 1.0365945891840493, Val Loss: 1.0528393762308612\n",
            "Epoch: 60, Loss: 1.0303085226826858, Val Loss: 1.0468210844904673\n",
            "Epoch: 61, Loss: 1.0241765274452692, Val Loss: 1.0409469551530761\n",
            "Epoch: 62, Loss: 1.0181939410676406, Val Loss: 1.0352123498212498\n",
            "Epoch: 63, Loss: 1.0123540237754955, Val Loss: 1.0296099193745776\n",
            "Epoch: 64, Loss: 1.0066490732157407, Val Loss: 1.0241362663556586\n",
            "Epoch: 65, Loss: 1.0010752328193138, Val Loss: 1.0187882287998864\n",
            "Epoch: 66, Loss: 0.9956293484356011, Val Loss: 1.0135597448552678\n",
            "Epoch: 67, Loss: 0.9903065381246755, Val Loss: 1.0084467701733935\n",
            "Epoch: 68, Loss: 0.9851020457756128, Val Loss: 1.0034458149799645\n",
            "Epoch: 69, Loss: 0.980011214505573, Val Loss: 0.9985532559901936\n",
            "Epoch: 70, Loss: 0.9750298652218022, Val Loss: 0.9937635686446038\n",
            "Epoch: 71, Loss: 0.9701545580003533, Val Loss: 0.989074739884703\n",
            "Epoch: 72, Loss: 0.9653815841808266, Val Loss: 0.9844827178143901\n",
            "Epoch: 73, Loss: 0.9607073824610334, Val Loss: 0.9799841105625446\n",
            "Epoch: 74, Loss: 0.9561288915913985, Val Loss: 0.9755748082698998\n",
            "Epoch: 75, Loss: 0.9516415218532928, Val Loss: 0.9712512614923944\n",
            "Epoch: 76, Loss: 0.9472418858379367, Val Loss: 0.967011660919475\n",
            "Epoch: 77, Loss: 0.9429273923088697, Val Loss: 0.96285352484889\n",
            "Epoch: 78, Loss: 0.9386963513112413, Val Loss: 0.9587757084832804\n",
            "Epoch: 79, Loss: 0.9345465759073687, Val Loss: 0.9547752018577331\n",
            "Epoch: 80, Loss: 0.9304741426784662, Val Loss: 0.9508477925058414\n",
            "Epoch: 81, Loss: 0.9264763596652326, Val Loss: 0.9469915297968649\n",
            "Epoch: 82, Loss: 0.9225513842606103, Val Loss: 0.9432046757326298\n",
            "Epoch: 83, Loss: 0.9186975427833878, Val Loss: 0.9394854809872017\n",
            "Epoch: 84, Loss: 0.9149130233851769, Val Loss: 0.9358322460424108\n",
            "Epoch: 85, Loss: 0.9111956688159375, Val Loss: 0.9322432777431071\n",
            "Epoch: 86, Loss: 0.9075428902634385, Val Loss: 0.9287162546231332\n",
            "Epoch: 87, Loss: 0.9039526382004144, Val Loss: 0.9252493817979718\n",
            "Epoch: 88, Loss: 0.9004235615879683, Val Loss: 0.921841605060914\n",
            "Epoch: 89, Loss: 0.8969544106114125, Val Loss: 0.9184908404364263\n",
            "Epoch: 90, Loss: 0.8935432421059184, Val Loss: 0.9151956121641309\n",
            "Epoch: 91, Loss: 0.890187350468061, Val Loss: 0.911953811367873\n",
            "Epoch: 92, Loss: 0.8868859790362664, Val Loss: 0.9087644584188671\n",
            "Epoch: 93, Loss: 0.8836376384836729, Val Loss: 0.9056260604777364\n",
            "Epoch: 94, Loss: 0.8804413044738477, Val Loss: 0.9025375424910861\n",
            "Epoch: 95, Loss: 0.8772954173998179, Val Loss: 0.8994971802671926\n",
            "Epoch: 96, Loss: 0.8741988015802125, Val Loss: 0.8965039759669955\n",
            "Epoch: 97, Loss: 0.8711501068956474, Val Loss: 0.8935570869319553\n",
            "Epoch: 98, Loss: 0.8681482075675357, Val Loss: 0.8906547991584216\n",
            "Epoch: 99, Loss: 0.8651913783237158, Val Loss: 0.8877963556445277\n",
            "Epoch: 100, Loss: 0.8622789909527488, Val Loss: 0.8849810385775839\n",
            "Epoch: 101, Loss: 0.8594102639926156, Val Loss: 0.8822078494852699\n",
            "Epoch: 102, Loss: 0.8565834449083656, Val Loss: 0.8794756140927731\n",
            "Epoch: 103, Loss: 0.8537979569284173, Val Loss: 0.8767836402914845\n",
            "Epoch: 104, Loss: 0.8510539541576303, Val Loss: 0.8741304272467165\n",
            "Epoch: 105, Loss: 0.8483494975910769, Val Loss: 0.871514730320385\n",
            "Epoch: 106, Loss: 0.8456840417023416, Val Loss: 0.8689361807089941\n",
            "Epoch: 107, Loss: 0.8430568396724413, Val Loss: 0.8663930500540653\n",
            "Epoch: 108, Loss: 0.8404666250093457, Val Loss: 0.8638840814231964\n",
            "Epoch: 109, Loss: 0.8379121523518124, Val Loss: 0.8614091345936054\n",
            "Epoch: 110, Loss: 0.8353928510796517, Val Loss: 0.8589681072817184\n",
            "Epoch: 111, Loss: 0.8329080505488526, Val Loss: 0.8565595006079267\n",
            "Epoch: 112, Loss: 0.8304567963785489, Val Loss: 0.8541830746833367\n",
            "Epoch: 113, Loss: 0.8280387853857639, Val Loss: 0.8518378417931702\n",
            "Epoch: 114, Loss: 0.8256532262627977, Val Loss: 0.8495228040921876\n",
            "Epoch: 115, Loss: 0.8232991779735475, Val Loss: 0.8472381965521872\n",
            "Epoch: 116, Loss: 0.8209762460273449, Val Loss: 0.8449836192230428\n",
            "Epoch: 117, Loss: 0.8186835195450242, Val Loss: 0.8427578251066984\n",
            "Epoch: 118, Loss: 0.8164203561058329, Val Loss: 0.8405604218765805\n",
            "Epoch: 119, Loss: 0.8141860527925079, Val Loss: 0.8383910437808739\n",
            "Epoch: 120, Loss: 0.811979622377762, Val Loss: 0.8362493300156889\n",
            "Epoch: 121, Loss: 0.8098007435971651, Val Loss: 0.8341349712416313\n",
            "Epoch: 122, Loss: 0.8076493593322899, Val Loss: 0.8320470291137805\n",
            "Epoch: 123, Loss: 0.805525039632083, Val Loss: 0.8299849724863094\n",
            "Epoch: 124, Loss: 0.8034271691668361, Val Loss: 0.8279481490293709\n",
            "Epoch: 125, Loss: 0.8013551250702511, Val Loss: 0.8259358067883061\n",
            "Epoch: 126, Loss: 0.7993083304535639, Val Loss: 0.8239475027232412\n",
            "Epoch: 127, Loss: 0.7972863261481287, Val Loss: 0.821982659968207\n",
            "Epoch: 128, Loss: 0.7952888820266673, Val Loss: 0.8200414055778935\n",
            "Epoch: 129, Loss: 0.7933159805729806, Val Loss: 0.8181232909618\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABVp0lEQVR4nO3dd3xV9f3H8de92XtvEhL2CCNMAReKIGoErdaBIq7WFqqWn/4UW22trXGVn1YtVlu1VlCrBVRQKjIFGQoECSOshAQyIAQyybzn98cJgQiBJCQ5Ge/n43EeyT33e+793G9L7tvv+Z7vsRmGYSAiIiJiEbvVBYiIiEjnpjAiIiIillIYEREREUspjIiIiIilFEZERETEUgojIiIiYimFEREREbGUwoiIiIhYytnqAhrC4XCQlZWFj48PNpvN6nJERESkAQzDoKioiMjISOz2+sc/2kUYycrKIjo62uoyREREpAkyMzPp0qVLvc+3izDi4+MDmB/G19fX4mpERESkIQoLC4mOjq79Hq9PuwgjJ0/N+Pr6KoyIiIi0M+ebYqEJrCIiImIphRERERGxlMKIiIiIWKpdzBkRERFpCYZhUFVVRXV1tdWltEtOTk44Oztf8LIbCiMiItIpVVRUkJ2dTWlpqdWltGuenp5ERETg6ura5NdQGBERkU7H4XCQlpaGk5MTkZGRuLq6alHNRjIMg4qKCo4cOUJaWho9e/Y858Jm56IwIiIinU5FRQUOh4Po6Gg8PT2tLqfd8vDwwMXFhQMHDlBRUYG7u3uTXkcTWEVEpNNq6n/JyynN0YeNeoWkpCSGDx+Oj48PoaGhTJ48mdTU1AYf/+GHH2Kz2Zg8eXJj6xQREZEOqlFhZNWqVUyfPp3169ezdOlSKisrGT9+PCUlJec9Nj09nUceeYRLLrmkycWKiIhIx9OoMLJkyRKmTZtG//79GTRoEO+++y4ZGRls2rTpnMdVV1czZcoUnn76abp163ZBBYuIiEjziI2N5eWXX7a6jAubwFpQUABAYGDgOdv94Q9/IDQ0lHvvvZdvvvnmvK9bXl5OeXl57ePCwsILKVNERKTDuPzyyxk8eHCzhIjvvvsOLy+vCy/qAjU5jDgcDh5++GHGjBlDfHx8ve3WrFnDP/7xD5KTkxv82klJSTz99NNNLa3BNnz5HhV7vyFkzFT6DNHpIxERaf8Mw6C6uhpn5/N/xYeEhLRCRefX5Cmw06dPJyUlhQ8//LDeNkVFRdx555289dZbBAcHN/i1Z82aRUFBQe2WmZnZ1DLPyTXl31xy9N/kb1/eIq8vIiLth2EYlFZUWbIZhtGgGqdNm8aqVat45ZVXsNls2Gw23n33XWw2G19++SVDhw7Fzc2NNWvWsG/fPiZNmkRYWBje3t4MHz6cr7/+us7r/fg0jc1m4+9//zs33HADnp6e9OzZk88++6w5u/msmjQyMmPGDBYtWsTq1avp0qVLve327dtHeno6iYmJtfscDof5xs7OpKam0r179zOOc3Nzw83NrSmlNUp5cH8o+QaXIykt/l4iItK2naispt9T/7XkvXf8YQKeruf/Sn7llVfYvXs38fHx/OEPfwBg+/btADz++OO89NJLdOvWjYCAADIzM7nmmmv405/+hJubG++99x6JiYmkpqYSExNT73s8/fTTvPDCC7z44ou8+uqrTJkyhQMHDpx3SsaFaNTIiGEYzJgxgwULFrB8+XLi4uLO2b5Pnz5s27aN5OTk2u36669n7NixJCcnEx0dfUHFXyj36MEABBfvtrQOERGRhvDz88PV1RVPT0/Cw8MJDw/HyckJMOdnXnXVVXTv3p3AwEAGDRrEz3/+c+Lj4+nZsyfPPPMM3bt3P+9Ix7Rp07jtttvo0aMHzz77LMXFxWzcuLFFP1ejRkamT5/OvHnz+PTTT/Hx8SEnJwcwO8fDwwOAqVOnEhUVRVJSEu7u7mfMJ/H39wc45zyT1hLeezisgS7VmZSXleDmbv0kHhERsYaHixM7/jDBsve+UMOGDavzuLi4mN///vcsXryY7OxsqqqqOHHiBBkZGed8nYEDB9b+7uXlha+vL4cPH77g+s6lUWFkzpw5gDmT93TvvPMO06ZNAyAjI6PdrGgXFtWN43jjbytm3+4tdB94sdUliYiIRWw2W4NOlbRVP74q5pFHHmHp0qW89NJL9OjRAw8PD2666SYqKirO+TouLi51HttsttopFi2lUb3ekAk2K1euPOfz7777bmPeskXZ7HYyXXvgX5HMsX2bQWFERETaOFdXV6qrq8/bbu3atUybNo0bbrgBMEdK0tPTW7i6pmkfQxgtqNi/NwCO7B8srkREROT8YmNj2bBhA+np6eTl5dU7atGzZ0/mz59PcnIyW7du5fbbb2/xEY6m6vRhxB5hnhvzOb7T4kpERETO75FHHsHJyYl+/foREhJS7xyQ2bNnExAQwOjRo0lMTGTChAkMGTKklattGJvR0IubLVRYWIifnx8FBQX4+vo262vv3rqOXguupggPvJ/KwtZO5ruIiEjTlZWVkZaWRlxcXJNvey+mc/VlQ7+/O/03b0zvwZQbzvhwgrxDe6wuR0REpNPp9GHE3d2DDCdz8Zec1O8srkZERKTz6fRhBCDP25zEWpaZbG0hIiIinZDCCFAV2h8At6M7LK5ERESk81EYAby6JgAQVqJl4UVERFqbwgjQpc8IAMKMI5QV5llcjYiISOeiMAKEBIdwkFAAsnZpEquIiEhrUhjBXHc/y70nAAXpWyyuRkREpHNRGKlxIrAvALacbRZXIiIi0nJiY2N5+eWXrS6jDoWRGs5RgwDwL9xlcSUiIiKdi8JIjZCewwCIqjqAUVFqcTUiIiKdh8JIjdi43uQZfrhQrZVYRUSkTXrzzTeJjIw84+67kyZN4p577mHfvn1MmjSJsLAwvL29GT58OF9//bVF1TacwkgNVxcn9rv1ASAv9VuLqxERkVZnGFBRYs3WwHvW3nzzzRw9epQVK1bU7svPz2fJkiVMmTKF4uJirrnmGpYtW8aWLVu4+uqrSUxMrPfOvm2Fs9UFtCXFQQMhewMc2mR1KSIi0toqS+HZSGve+4kscPU6b7OAgAAmTpzIvHnzuPLKKwH45JNPCA4OZuzYsdjtdgYNGlTb/plnnmHBggV89tlnzJgxo8XKv1AaGTmNa1dz8bOQQl1RIyIibdOUKVP4z3/+Q3l5OQBz587l1ltvxW63U1xczCOPPELfvn3x9/fH29ubnTt3amSkPYmKHwPrIbw6h6rCwzj7hlpdkoiItBYXT3OEwqr3bqDExEQMw2Dx4sUMHz6cb775hv/7v/8D4JFHHmHp0qW89NJL9OjRAw8PD2666SYqKipaqvJmoTBymq6RkaQZkcTZssja8S0xF022uiQREWktNluDTpVYzd3dnRtvvJG5c+eyd+9eevfuzZAhQwBYu3Yt06ZN44YbbgCguLiY9PR0C6ttGJ2mOY3dbiPTsx8ABXvXWVyNiIjI2U2ZMoXFixfz9ttvM2XKlNr9PXv2ZP78+SQnJ7N161Zuv/32M668aYsURn6kLMy8g69rrpaFFxGRtumKK64gMDCQ1NRUbr/99tr9s2fPJiAggNGjR5OYmMiECRNqR03aMp2m+RHvbiMgHSKKd5iXWtlsVpckIiJSh91uJyvrzPktsbGxLF++vM6+6dOn13ncFk/baGTkR+LiR1JuuOBrFFGWu8fqckRERDo8hZEfCQ/wJdUeB0DW9jUWVyMiItLxKYz8iM1mI9cnHoAT6RstrkZERKTjUxg5i+oIc7KP15FkawsRERHpBBRGziKw1ygAIsv2QFW5xdWIiIh0bAojZ9GrzwCOGj64UkXRAV3iKyLSURkNvEGd1K85+lBh5Cz8vdzY7dwLgNwday2uRkREmpuLiwsApaWlFlfS/p3sw5N92hRaZ6QeRwMGQd4mHAe0EquISEfj5OSEv78/hw8fBsDT0xOb1pVqFMMwKC0t5fDhw/j7++Pk5NTk12pUGElKSmL+/Pns2rULDw8PRo8ezfPPP0/v3r3rPeatt97ivffeIyUlBYChQ4fy7LPPMmLEiCYX3RqcY8dA3tuE5G/S4mciIh1QeHg4QG0gkabx9/ev7cumalQYWbVqFdOnT2f48OFUVVXxxBNPMH78eHbs2IGX19lvLrRy5Upuu+02Ro8ejbu7O88//zzjx49n+/btREVFXVDxLSlm4CWUf+dMgCOfqrx9OIf0sLokERFpRjabjYiICEJDQ6msrLS6nHbJxcXlgkZETrIZFzDz5MiRI4SGhrJq1SouvfTSBh1TXV1NQEAAr732GlOnTm3QMYWFhfj5+VFQUICvr29Ty22UaofB1qdHMsSWysFLX6LLFfe3yvuKiIh0FA39/r6gCawFBQUABAYGNviY0tJSKisrz3lMeXk5hYWFdbbW5mS3ccjPvGneib2rW/39RUREOosmhxGHw8HDDz/MmDFjiI+Pb/Bxjz32GJGRkYwbN67eNklJSfj5+dVu0dHRTS3zghgx5noj/ke+t+T9RUREOoMmh5Hp06eTkpLChx9+2OBjnnvuOT788EMWLFiAu7t7ve1mzZpFQUFB7ZaZmdnUMi9IRPxlVBs2QiqzMArPvDuiiIiIXLgmhZEZM2awaNEiVqxYQZcuXRp0zEsvvcRzzz3HV199xcCBA8/Z1s3NDV9f3zqbFeK7xbCLrgAc3b7KkhpEREQ6ukaFEcMwmDFjBgsWLGD58uXExcU16LgXXniBZ555hiVLljBs2LAmFWoFD1cn0jzN4FSQqjAiIiLSEhoVRqZPn87777/PvHnz8PHxIScnh5ycHE6cOFHbZurUqcyaNav28fPPP8+TTz7J22+/TWxsbO0xxcXFzfcpWlB51EUAeOZssLgSERGRjqlRYWTOnDkUFBRw+eWXExERUbt99NFHtW0yMjLIzs6uc0xFRQU33XRTnWNeeuml5vsULSi43+UAhJWlQWm+tcWIiIh0QI1a9KwhS5KsXLmyzuP09PTGvEWbM6B3T/Y5Iuhuz6Zoz1p8BiVaXZKIiEiHohvlnUeglyupbgMAOLpjpbXFiIiIdEAKIw1QHD4cAJeDummeiIhIc1MYaQCfXpcBEFayC8rbx8RbERGR9kJhpAH69xtAhiMEZ6op26el4UVERJqTwkgDRAd6sNnFvE9N3tb/WlyNiIhIx6Iw0gA2m42iiDEAuGVoZERERKQ5KYw0UOCAq3AYNkJO7IeiHKvLERER6TAURhpoeN/upBixAJTs+traYkRERDoQhZEGCvV1Z7v7EACObfvK4mpEREQ6DoWRRqiIMS/x9c1aAw1YjVZERETOT2GkEaIGXk6Z4YJv1VE4ssvqckRERDoEhZFGGN4zku+MPgAUbF9qcTUiIiIdg8JII/h5uLDPx1wavnSnJrGKiIg0B4WRxup2OQCBRzZCdaW1tYiIiHQACiON1HPgKI4aPrgZJzAyN1pdjoiISLunMNJIQ2ODWG/EA3Bsm5aGFxERuVAKI43k7uJEZuBoAIw9Wm9ERETkQimMNIFr3wk4DBtBhTuhMNvqckRERNo1hZEmuGhAX34wugFQuWuJxdWIiIi0bwojTdA3wocNLuYlvgU/LLa4GhERkfZNYaQJbDYb5XHjgJql4avKLa5IRESk/VIYaaJegy4mxwjA1XEC0tdYXY6IiEi7pTDSRGN6BrPKGAzoVI2IiMiFUBhpIh93Fw4GXwqAbc9/dRdfERGRJlIYuQCB8eMpN5zxPXEQ8vZYXY6IiEi7pDByAS6J78p6Rz8AKnZ+YXE1IiIi7ZPCyAXoHuLNZvcRAJSkKIyIiIg0hcLIBbDZbDh6XAWA7+Hv4cQxiysSERFpfxRGLtDggQnsdETjRDVG6pdWlyMiItLuKIxcoFHdg/ga81RN8Zb5FlcjIiLS/iiMXCBPV2fyoq8GwCNjFZQXWVyRiIhI+9KoMJKUlMTw4cPx8fEhNDSUyZMnk5qaet7jPv74Y/r06YO7uzsDBgzgiy861mTP+MEXsd8RjrNRAXuWWl2OiIhIu9KoMLJq1SqmT5/O+vXrWbp0KZWVlYwfP56SkpJ6j/n222+57bbbuPfee9myZQuTJ09m8uTJpKSkXHDxbcW4fuF8ZdRcVZO8wOJqRERE2hebYTR96dAjR44QGhrKqlWruPTSS8/a5pZbbqGkpIRFixbV7rvooosYPHgwb7zxRoPep7CwED8/PwoKCvD19W1quS3qN6/9kz/lPUilkwcuj6eBi4fVJYmIiFiqod/fFzRnpKCgAIDAwMB626xbt45x48bV2TdhwgTWrVtX7zHl5eUUFhbW2dq6noMv4aARjEv1Cdi33OpyRERE2o0mhxGHw8HDDz/MmDFjiI+Pr7ddTk4OYWFhdfaFhYWRk5NT7zFJSUn4+fnVbtHR0U0ts9WMj4/gv9XDASj7YaG1xYiIiLQjTQ4j06dPJyUlhQ8//LA56wFg1qxZFBQU1G6ZmZnN/h7NLdLfgz1BlwNg3/0lVFVYW5CIiEg70aQwMmPGDBYtWsSKFSvo0qXLOduGh4eTm5tbZ19ubi7h4eH1HuPm5oavr2+drT2IGTSWI4YfrlVFkL7a6nJERETahUaFEcMwmDFjBgsWLGD58uXExcWd95hRo0axbNmyOvuWLl3KqFGjGldpOzA+Por/Vg8DoOIHLYAmIiLSEI0KI9OnT+f9999n3rx5+Pj4kJOTQ05ODidOnKhtM3XqVGbNmlX7+KGHHmLJkiX8+c9/ZteuXfz+97/n+++/Z8aMGc33KdqIHqHebPYZaz7Y+TlUlVtbkIiISDvQqDAyZ84cCgoKuPzyy4mIiKjdPvroo9o2GRkZZGdn1z4ePXo08+bN480332TQoEF88sknLFy48JyTXtuziIFXkG0E4lpZCHu/trocERGRNu+C1hlpLe1hnZGTdmQVsuavD/Az58VU9pmEy63vWV2SiIiIJVplnRE5U98IHzb7XgmAffcS3atGRETkPBRGmpnNZqNvwiXsc0Tg5CiHXYutLklERKRNUxhpAdcNjuTT6jEAVCR/dJ7WIiIinZvCSAvoHuLNzuCrAHBOXwXFRyyuSEREpO1SGGkhQ4cMZ6ujG3ajGnYstLocERGRNkthpIVcNzCCz6pHAzpVIyIici4KIy2kS4AnByKuxmHYcM36DvL3W12SiIhIm6Qw0oIuTohnjaNmcbfkD6wtRkREpI1SGGlB1wyM4BPHZQBUbX4fHNUWVyQiItL2KIy0oFAfd0q6TaTA8MS5OAvSVlldkoiISJujMNLCrh8aVzuR1dgy1+JqRERE2h6FkRY2oX84i5zM5eGNnZ/DiePWFiQiItLGKIy0MHcXJ7oNHMMuRzT26nJI+Y/VJYmIiLQpCiOt4KZh0XxcfSkA1Zvft7gaERGRtkVhpBUMiQlgs994Kg0nnLI3w+FdVpckIiLSZiiMtAKbzcaVw/qzwjHY3LHlX5bWIyIi0pYojLSSG4Z04SPHWACqt8yFyjKLKxIREWkbFEZaSZS/B2Vdr+CQEYRT2THY8anVJYmIiLQJCiOt6KbhXfmg6goAjO/ftrgaERGRtkFhpBVNjI/gS5erqDScsGWuh9ztVpckIiJiOYWRVuTu4sRlQwfwlWOouUOjIyIiIgojre32kdHMrR4HgGPrh1BebHFFIiIi1lIYaWU9Qn2ojrmY/Y5w7BXFsO1jq0sSERGxlMKIBW4bGcvc6pr71Xz/NhiGxRWJiIhYR2HEAlfHh7PM9UrKDRdsOT9A5karSxIREbGMwogF3F2cGDe0L59WjzZ3bJhjbUEiIiIWUhixyK0jYnin+moAjB2fQcFBiysSERGxhsKIRXqEeuMfN4R11f2wGdWw8S2rSxIREbGEwoiFpo2J5Z3qCQAYm96FilJrCxIREbGAwoiFxvUNY6fPGDIcIdjKjsMPH1pdkoiISKtTGLGQk93GnWO68c+ToyPr39BlviIi0uk0OoysXr2axMREIiMjsdlsLFy48LzHzJ07l0GDBuHp6UlERAT33HMPR48ebUq9Hc5Ph0Xzqf0Kig13bHmpsG+51SWJiIi0qkaHkZKSEgYNGsTrr7/eoPZr165l6tSp3HvvvWzfvp2PP/6YjRs3cv/99ze62I7I39OVqxJ68XH1ZeaOdQ3rVxERkY7CubEHTJw4kYkTJza4/bp164iNjeXBBx8EIC4ujp///Oc8//zzjX3rDmva6Fju/+5qpjp9hdO+ZZCzDcIHWF2WiIhIq2jxOSOjRo0iMzOTL774AsMwyM3N5ZNPPuGaa66p95jy8nIKCwvrbB1Z73AfouL68aVjpLlj7V+sLUhERKQVtXgYGTNmDHPnzuWWW27B1dWV8PBw/Pz8znmaJykpCT8/v9otOjq6pcu03N1jYplTlQiAkfIfOHbA4opERERaR4uHkR07dvDQQw/x1FNPsWnTJpYsWUJ6ejoPPPBAvcfMmjWLgoKC2i0zM7Oly7TcuL5hlAbF8011vLkI2vq/Wl2SiIhIq2j0nJHGSkpKYsyYMTz66KMADBw4EC8vLy655BL++Mc/EhERccYxbm5uuLm5tXRpbYrdbuO+S+L426eJXOKUgrH5PWyXPQaegVaXJiIi0qJafGSktLQUu73u2zg5OQFgaE2NOn4ypAs73YeQ4ojFVlmqJeJFRKRTaHQYKS4uJjk5meTkZADS0tJITk4mIyMDME+xTJ06tbZ9YmIi8+fPZ86cOezfv5+1a9fy4IMPMmLECCIjI5vnU3QQ7i5O3DUmjjdOzh3Z+DctES8iIh1eo8PI999/T0JCAgkJCQDMnDmThIQEnnrqKQCys7NrgwnAtGnTmD17Nq+99hrx8fHcfPPN9O7dm/nz5zfTR+hY7ryoKyucLuKAIxRb6VHY9K7VJYmIiLQom9EOzpUUFhbi5+dHQUEBvr6+VpfT4p76NIXyje/yvMtb4B0OD20FF3eryxIREWmUhn5/6940bdB9F3djoeMSDhrBUJwDW/5ldUkiIiItRmGkDYoJ8uSqAdG1c0dY839QVW5tUSIiIi1EYaSNmj62Bx9XX0aOEQCFhyB5ntUliYiItAiFkTaqb4Qvl/Y7fXRkNlRXWluUiIhIC1AYacNmjO3BB9VXcMTwg+MZsPVDq0sSERFpdgojbdigaH9G9Izkb1XXmTtWvwhVFdYWJSIi0swURtq4X13Rk/erx9WMjhzQlTUiItLhKIy0cSPiAhkYF8FrVZPNHatfhMoTltYkIiLSnBRG2oEHr+jJB9VXcMgIhqJs+P5tq0sSERFpNgoj7cCYHkEMig3llaobzB3fzIbyYmuLEhERaSYKI+2AzWZj5lW9mV99CelGOJTmwYY5VpclIiLSLBRG2olR3YMY0T2M2ZU/MXesfRVOHLO2KBERkWagMNKOzLyqF587RpHqiIbyAnOZeBERkXZOYaQdGRYbyCW9wniu6lZzx/o34HimtUWJiIhcIIWRdmbmVb1Y4RjMOkc/qC6HFc9aXZKIiMgFURhpZwZH+zOubzhJlbeZO7Z+ADnbrC1KRETkAiiMtEOPTuhNCt35vPoiwIClv7O6JBERkSZTGGmHeof7cOOQLrxYdQtVOMO+ZbBvhdVliYiINInCSDv166t6keMUwb+qrjR3LH0SHNXWFiUiItIECiPtVJS/B9NGx/KXqhsoxsucN7LlfavLEhERaTSFkXbsl5d3p8o9kP+rrFkmfvkzUFZobVEiIiKNpDDSjvl7uvKLy7vzXvV4DtgioeQIfPOS1WWJiIg0isJIO3fPmDhC/Lx5uvx2c8f6OZC/39qiREREGkFhpJ1zd3HisYl9WO5IYI0xEKor4KsnrS5LRESkwRRGOoDrB0UyODqA31fcQTVOsGsR7F9pdVkiIiINojDSAdhsNp68rh97jS78q2qcufOLR6GqwtrCREREGkBhpIMY2jWA6wZGMLvqJgrs/pC3G9b/1eqyREREzkthpAN5fGIfypx9+ENZzV19V70ABYesLUpEROQ8FEY6kC4Bnvzskm7Md1zMVlsfqCyBr35jdVkiIiLnpDDSwfxybHci/Dx5vOwuHNhh+wLdt0ZERNo0hZEOxtPVmd9e14+dRlfed4w3d37xCFSWWVuYiIhIPRodRlavXk1iYiKRkZHYbDYWLlx43mPKy8v5zW9+Q9euXXFzcyM2Npa33367KfVKA0yMD2dMjyBeqvgJx50C4eheWDPb6rJERETOqtFhpKSkhEGDBvH66683+Jif/vSnLFu2jH/84x+kpqbywQcf0Lt378a+tTSQzWbj94n9KbV788SJO8yd38yGI6nWFiYiInIWzo09YOLEiUycOLHB7ZcsWcKqVavYv38/gYGBAMTGxjb2baWReob5cPeYWN76xsE6p2GMqv4ePn8Ypi0Gu87OiYhI29Hi30qfffYZw4YN44UXXiAqKopevXrxyCOPcOLEiXqPKS8vp7CwsM4mjffglT0J83XnkZI7qbS7Q8a3sOVfVpclIiJSR4uHkf3797NmzRpSUlJYsGABL7/8Mp988gm//OUv6z0mKSkJPz+/2i06Orqly+yQfNxd+F1ifw4RwouVN5k7lz4JRbnWFiYiInKaFg8jDocDm83G3LlzGTFiBNdccw2zZ8/mn//8Z72jI7NmzaKgoKB2y8zMbOkyO6yJ8eGM7R3CPyonsN+5B5QVmFfXiIiItBEtHkYiIiKIiorCz8+vdl/fvn0xDIODBw+e9Rg3Nzd8fX3rbNI0NpuNP0yKx8XFhRkl9+CwOcPOz8z1R0RERNqAFg8jY8aMISsri+Li4tp9u3fvxm6306VLl5Z+ewGiAz156Mpe7DBi+bsxydy5+BEoOWptYSIiIjQhjBQXF5OcnExycjIAaWlpJCcnk5GRAZinWKZOnVrb/vbbbycoKIi7776bHTt2sHr1ah599FHuuecePDw8mudTyHndd0kcvcN8eKnserLd4qA0D5Y8ZnVZIiIijQ8j33//PQkJCSQkJAAwc+ZMEhISeOqppwDIzs6uDSYA3t7eLF26lOPHjzNs2DCmTJlCYmIif/nLX5rpI0hDuDjZSfrJACptLvy88B4Mmx22fQy7vrC6NBER6eRshmEYVhdxPoWFhfj5+VFQUKD5IxfomUU7+MeaNP7o9W/uqF4I3mHwy/XgGWh1aSIi0sE09Ptbq191Mv8zvhfRgR48UzKZw+6xUJwLi34NbT+TiohIB6Uw0sl4ujrz3I0DKceVewruM6+u2bEQUv5jdWkiItJJKYx0QmN6BHPr8GhSjG780+Vmc+fi/4HCLGsLExGRTklhpJOadU1fwn3d+VPhRA559oGy4/DpDJ2uERGRVqcw0kn5ebjw/E0DqcKZqcfupdrJDfYtg+/+bnVpIiLSySiMdGKX9Qrh9pEx7DOi+IvtDnPnV7+F3B3WFiYiIp2Kwkgn98Q1fYkO9OCV4itI9bkIqsrgP/dCZZnVpYmISCehMNLJebs58+JNgwAbtx+5iwq3IDi8A77+ndWliYhIJ6EwIlzULYh7L47jKH78T+UD5s4Nb8Du/1pbmIiIdAoKIwLAoxN60yfch89L+/OVzw3mzoW/0OW+IiLS4hRGBAB3FydevnUwrs52fnVkEvk+vaH0KPznPqiusro8ERHpwBRGpFafcF9mTexDOa7ceuwBHC5ecGAtrEyyujQREenAFEakjmmjY7m0Vwi7q8J43uWX5s5v/gx7l1lbmIiIdFgKI1KHzWbjpZsGEuTlyt/yE9gYNAkwYP7PoDDb6vJERKQDUhiRM4T6uvPnnw4C4M5DN1Do1wdK8+DjaVBdaW1xIiLS4SiMyFld3juUn1/WjXJcua3gFzhcfSBzPXz1pNWliYhIB6MwIvV6ZHxvEmL82V4WwnPuD5s7N8yBbZ9YWpeIiHQsCiNSLxcnO6/eloCvuzNvHu7LmvC7zCc++xUc3mltcSIi0mEojMg5dQnw5KWbzfkjU9OvIi90NFSWwoe3w4ljFlcnIiIdgcKInNf4/uH8/LJuOLAzOeduKr2jIH+/JrSKiEizUBiRBnl0fG9GxgVysMKLX/G/GC5esH8l/PcJq0sTEZF2TmFEGsTZyc6rtycQ4uPGkrwQ3g593Hxi45vw3T+sLU5ERNo1hRFpsFAfd167LQEnu41n9nVnY7cZ5hNfPAr7V1lbnIiItFsKI9IoI7sF8dR1/QC4decocromglEN/54KR/dZXJ2IiLRHCiPSaFNHdeW2EdE4DBvXpd9CWWgClB2HD26FsgKryxMRkXZGYUQazWaz8fT18QyPDSCv3M4dxQ/i8I6AvN3wyT3gqLa6RBERaUcURqRJXJ3tzLljKJF+7nyf78bvvX+L4ewBe7+Gr35rdXkiItKOKIxIkwV7u/HWXcPwcHHivfQA5sf8xnxi/V9h3evWFiciIu2GwohckP6RfrUrtP7Pjm5s6/Ow+cR/n9A9bEREpEEURuSCXTswggev6AHAT7aN5HC/aeYTCx6AfSusK0xERNoFhRFpFg+P68X4fmFUVBtcm3otpT2vB0clfHQHZG+1ujwREWnDGh1GVq9eTWJiIpGRkdhsNhYuXNjgY9euXYuzszODBw9u7NtKG2e32/i/WwbTN8KXIyWV3JA1lcqYi6GiGN6/CfLTrC5RRETaqEaHkZKSEgYNGsTrrzduguLx48eZOnUqV155ZWPfUtoJLzdn3r17OFH+HqQereDu0odwhMZDyWF4/0YoPmJ1iSIi0gY1OoxMnDiRP/7xj9xwww2NOu6BBx7g9ttvZ9SoUY19S2lHwnzdeffu4fi6O7PmYCX/6/4Uhn+MeZffeT+F8mKrSxQRkTamVeaMvPPOO+zfv5/f/e53DWpfXl5OYWFhnU3aj55hPvz9ruG4Otv5ZHcVL4c/h+EZBFmb4d93QlW51SWKiEgb0uJhZM+ePTz++OO8//77ODs7N+iYpKQk/Pz8arfo6OgWrlKa24i4QF65ZTA2G7ySDJ/0ng0unrBvOXw8DaoqrC5RRETaiBYNI9XV1dx+++08/fTT9OrVq8HHzZo1i4KCgtotMzOzBauUljJxQETtTfUeXefC6qGvgrM7pH4B/7kXqqssrlBERNoCm2EYRpMPttlYsGABkydPPuvzx48fJyAgACcnp9p9DocDwzBwcnLiq6++4oorrjjv+xQWFuLn50dBQQG+vr5NLVcskvTFTv62ej/OdhsLxpcy4JtfQHUFxP8EbnwL7E7nfxEREWl3Gvr93bDzJk3k6+vLtm3b6uz761//yvLly/nkk0+Ii4trybeXNuKxq/uQXVDGZ1uzuGW5N59d9Vd6rHgAUv4DTq4w6a9g15I3IiKdVaPDSHFxMXv37q19nJaWRnJyMoGBgcTExDBr1iwOHTrEe++9h91uJz4+vs7xoaGhuLu7n7FfOi673caLNw8kv6SCNXvzuGGZL4vHvU7Msl/C1g/AyQWue0WBRESkk2r0X//vv/+ehIQEEhISAJg5cyYJCQk89dRTAGRnZ5ORkdG8VUq75+bsxJtThzI8NoCisiomLQ8k68pXwWaHze/Bl49C088YiohIO3ZBc0Zai+aMdBxFZZXc8fcNbD1YQLC3G19cfpDQrx8GDBj5C7g6CWw2q8sUEZFm0NDvb42LS6vycXfhn/eMoG+EL3nF5Uz6Jpr8K18yn9wwBxbPBIfD2iJFRKRVKYxIq/P3dOVf946gR6g32QVlTFrXjYKrZgM2+P5tWPiALvsVEelEFEbEEsHebsy9byRdgzzJzD/BDeu7U3jNHLA5wQ8fwcd3aaVWEZFOQmFELBPm687c+0YS5e/B/iMl3Lw2ioLr3zEv9921CD64DSpKrS5TRERamMKIWKpLgCdz7xtJqI8bqblF3Ljcj/xJ79csHb8M3v8JlOneRCIiHZnCiFguNtiLj34+ikg/d/YdKeGG/7pyePIH4OYLGd/Ce9dDab7VZYqISAtRGJE2Ia4mkMQEenLgaCk3fO4ga9K/wSMQsrbAO9dAwSGryxQRkRagMCJtRnSgJ//++Si6hXhx6PgJJi8o4cCkT8A7HI7shL+Pg5wUq8sUEZFmpjAibUq4nzsf/WwUfcJ9OFxUzo0f57MncQEE94aiLHj7ati3wuoyRUSkGSmMSJsT4uPGB/dfRHyUL0dLKrjpw4OkXP0xdL0YKopg7k2QPM/qMkVEpJkojEibFODlytz7LmJIjD8FJyq59V+7+Hb0mxD/E3BUwcJfwKoXdD8bEZEOQGFE2iw/Dxf+de9IRnULori8irve28qn3Z+GMQ+bDVb8CT5/EKorLa1TREQujMKItGlebs68e89wrh0YQWW1wUMf/cCbblMxrvnzqTv+fnArlBVYXaqIiDSRwoi0eW7OTrx6awL3jIkD4NkvdvFM7mgcP51rLo6292t460rI22txpSIi0hQKI9Iu2O02nkrsx2+v7QvA22vT+NWWcMrvXAS+UXB0D7x1BexZanGlIiLSWAoj0q7cd0k3/nJbAi5ONhb/kM3ULysonLoUoi+C8gKYezOsfUUTW0VE2hGFEWl3rh8UyT/vHoGPmzMb0vK56V97ybz+IxhyF2DA0qdg/v1QecLqUkVEpAEURqRdGt0jmI9+PopQHzd25xYz6Y3v2ND/KbjmJbA7w7aPzQXSCg5aXaqIiJyHwoi0W/0iffl0xhjio3zJL6ngjrc38iET4M6F5j1tspPhzcth/yqLKxURkXNRGJF2LcLPg49/Prr20t/H52/j6ZRAqu5bAWHxUHIE/jXZXCDNUW11uSIichYKI9Luebg68dptCcy8qhcA76xN5+6FuRRM+QIS7gDDYS6Q9v5PoPiIxdWKiMiPKYxIh2Cz2Xjwyp7MmTIEDxcnvtmTxw1vbmH/6Odh8hxw9oD9K+Bvl0D6WqvLFRGR0yiMSIcycUAEn/xiFJF+7uzPK2HS62tZ6nol/GxFzZ1/s+GfifDNbHA4rC5XRERQGJEOqH+kH5/OuJhhXQMoKqvi/ve+57lNNqru/RoG3gJGNSx7Gub9VKdtRETaAIUR6ZBCfNyYd/9F3D0mFoA3Vu3j9ve2c/jKVyDxL+DkBnuXwpxRkLrE2mJFRDo5hRHpsFyd7fwusT+v3z4EL1cnNqblc82ra1nnfx3cvxxC+5lX23xwCyz6NVSUWF2yiEinpDAiHd61AyP47FcX0zvMh7zicqb8fT1zdnnguG85XDTdbPT92/C3S+HQZmuLFRHphBRGpFPoHuLNwuljuHFIFA4Dnl+yi/vnpZB/ye/NRdJ8IuHoXvjHVbD6RaiusrpkEZFOQ2FEOg0PVyf+fPMgkm4cgKuznWW7DjPxldWsNQbAL9ZC/xvAUQXL/wjvXgN5e60uWUSkU1AYkU7FZrNx24gY5v9iNN1CvMgtLOeOf2wgaWUuFZP/ATe8CW6+kLkB3hhj3gFYoyQiIi1KYUQ6pfgoPxb96mJuHxmDYcDfVu/nhjnfsjfiWnOUpNtYqCoz7wD8j6sgd4fVJYuIdFiNDiOrV68mMTGRyMhIbDYbCxcuPGf7+fPnc9VVVxESEoKvry+jRo3iv//9b1PrFWk2nq7OPHvDAP5251ACPF3YnlXIda9+w7xUMO6YD5NeBzc/yNpsTm5d+TxUVVhdtohIh9PoMFJSUsKgQYN4/fXXG9R+9erVXHXVVXzxxRds2rSJsWPHkpiYyJYtWxpdrEhLmNA/nCUPX8qYHkGUVTp4YsE2fv7+ZvJ63gzTN0Dva8BRCSufNe8CrCtuRESalc0wDKPJB9tsLFiwgMmTJzfquP79+3PLLbfw1FNPNah9YWEhfn5+FBQU4Ovr24RKRc7P4TD4x5o0XvjvLiqrDQK9XPnDpP5cNyACUv4DX/4vlB4Fmx1GPgCXzwJ3/f9RRKQ+Df3+bvU5Iw6Hg6KiIgIDA+ttU15eTmFhYZ1NpKXZ7Tbuv7QbC6ePoU+4D/klFcyYt4VfzttMXlwiTN8I8T8x7wK8/q/w+ghImQ9Nz/MiIoIFYeSll16iuLiYn/70p/W2SUpKws/Pr3aLjo5uxQqls+sf6cdnMy7mwSt74my38cW2HK6avYrP91Zg/OQfcMd/ILCbedO9T+6G92+Eo/usLltEpN1q1dM08+bN4/777+fTTz9l3Lhx9bYrLy+nvLy89nFhYSHR0dE6TSOtLuVQAY9+8gM7s83Ruav7h/PM5HhC3A1Y+7J599/qcvNeNxf/2txc3K0tWkSkjWhzp2k+/PBD7rvvPv7973+fM4gAuLm54evrW2cTsUJ8lB+fTh/DQzWjJEu25zD+/1bx8dYjGJc9Br9cB92vMAPJqufgrxfBri906kZEpBFaJYx88MEH3H333XzwwQdce+21rfGWIs3G1dnOr6/qxaczxtA3wpdjpZU8+skP3PrmevZWh8Ed8+Hmd8EnAo6lwYe3wb8ma20SEZEGanQYKS4uJjk5meTkZADS0tJITk4mIyMDgFmzZjF16tTa9vPmzWPq1Kn8+c9/ZuTIkeTk5JCTk0NBQUHzfAKRVmLOJRnDY1f3wd3Fzoa0fCa+sprZS3dT1ut6mPGdeZrGyRX2rzRXcF00E0qOWl26iEib1ug5IytXrmTs2LFn7L/rrrt49913mTZtGunp6axcuRKAyy+/nFWrVtXbviF0aa+0NZn5pTz1aQorUo8AEBvkyR8nD+DinsGQn2au3LrzM7Oxux9c9jgMvw+cXS2sWkSkdTX0+/uCJrC2FoURaYsMw+DLlBye/nw7uYXmhOvJgyN54pq+hPq6Q9o3sGQW5G4zDwjqAeN+D32uA5vNusJFRFqJwohIKykqq+TPX+3mn+vSMQzwcnVixhU9uefiWNzswJb3YfkzUGKOotBlOIx7GmLHWFq3iEhLUxgRaWU/HDzOU59uJznzOGCeuvnttf24sm8otvIi+PYvsO51qCw1D+g5Acb9DsL6W1e0iEgLUhgRsYDDYbBgyyGeW7KLI0XmqZtLe4Xw1HX96BHqDUU5sOoF2PQuGNWADQbdCmOfAP8YS2sXEWluCiMiFiour+K15Xt5e00aFdUOnO027hody4NX9MTP08VcsXX5M7B9gXmAkysMuQsumQm+kdYWLyLSTBRGRNqA9LwS/rh4J1/vzAXAz8OFGWN7cOeorri7OJl3AP7695BWc8WZkxsMu9u8RNgn3LrCRUSagcKISBuyavcRnl28k9TcIgCi/D14ZEIvJg2Kwm4D0lbDyiTIWGce4OwOw+6BMQ+DT5hldYuIXAiFEZE2ptph8J/NB5n91W5yCssA6B/py6yJfc31SQzDXCxtZRJkbjAPcvaA4ffC6AcVSkSk3VEYEWmjTlRU8/baNN5YuY+i8ioALukZzP9O6MOALn5mKNm33AwlB78zD3Jyg4Q7YMyDEBBrXfEiIo2gMCLSxuWXVPDq8j28v/4AldXmP8MJ/cP49VW96BPua4aSvcvMG/CdDCU2JxhwszmnJLSPhdWLiJyfwohIO3HgaAkvf72HhcmHMAxzcdbrBkby8LiedA/xNkNJ+hr45s+wf8WpA/tcZ159EzXUuuJFRM5BYUSkndmTW8TLX+9h8bZsAOw2uCGhCw9d2ZOYIE+z0aFN8M1s2LXo1IGxl8DoX0GPq8DeKjfiFhFpEIURkXZqe1YB/7d0T+3lwM52GzcOieIXl/cgLtjLbHR4J6x5GbZ9XLN4GhDcC0ZNh4G3gou7NcWLiJxGYUSknUvOPM7spbtZvdu8p43dBomDIpk+tge9wnzMRsczYcMbsOmfUGFeNoxnMIy437xLsFewRdWLiCiMiHQYmzOO8fryvSzbdbh239X9w5lxRQ/io/zMHWWFsPk9M5gUZJr7nN1hwE0w/H6IHNz6hYtIp6cwItLBpBwq4K8r9/JlSg4n/9WO7R3CLy7vwfDYAGw2G1RXwY6FsO41yNpy6uAuI8zRkn6TwNnNkvpFpPNRGBHpoPbkFvHXlfv4NPkQjpp/vYOj/fn5pd0Y3z8cJ7vNvAIncwNsfAt2fAqOSrOhV4h5D5xhd4NfF+s+hIh0CgojIh1cel4Jf1u9n/9sPkhFlQOA2CBP7r2kGzcP7WLe+wagKBc2/xO+fweKssx9Nifoc415CifuUvN6YhGRZqYwItJJHCkq57116fxr/QGOl5ojIIFerkwd1ZU7L+pKkHfNaZnqSti1GL77O6R/c+oFgnubk10H3gweARZ8AhHpqBRGRDqZ0ooq/v1dJn9fk8bBYycAcHW2M2lQJNPGxNI/0u9U49wdZijZ+iFUlpj7nN2hbyIk3GmuXaI1S0TkAimMiHRSVdUOvkzJ4e/f7GfrwYLa/SPiArlnTCzj+obh7FQTNMoKzECy6Z9wePupFwmINe+FM3gK+Ea27gcQkQ5DYUSkkzMMg80Zx3n323S+3JZNVc1s1yh/D6aO6sotw6Px93Q92RiyNsPmf0HKf6C80Nxvs0OPceZoSa+rwdnVok8jIu2RwoiI1MopKONf69OZtyGDYzXzSlyd7Vw3MIIpI2MYElNzaTBARal5Bc7m9yDj21Mv4hkMg241R0xC+1rwKUSkvVEYEZEzlFVW81lyFu9+m86O7MLa/X3CfZgyMoZJCVH4urucOiBvL2z5F2z9AIpzT+0PHwADb4H4m8A3ohU/gYi0JwojIlIvwzBIzjzOvA0ZfP5DFmWV5qXBHi5OTBocyZSRXRnQ5bQJr9WVsGcpbHkf9nx1at0SbNDtMjOY9LkO3PXvU0ROURgRkQYpKK1k/paDzN2Qwd7DxbX7B0T5MWVkDNcPjsTT1fnUAaX5sH0B/PBvyFx/ar+zh7l2ycBboPsV4HTaCIuIdEoKIyLSKIZh8F36MeZuOMCX23KoqDZHS7zdnLl2QAQ3DevCsK6nzS0ByE+DbZ/ADx/B0T2n9nsGQf8bof8NEHMR2J1a+dOISFugMCIiTZZfUsEnmzKZtyGD9KOltftjgzz5yZAu3Di0C1H+HqcOMAzzXjg//Nu8Gqfk1E398A4374nTfzJEX6T1S0Q6EYUREblgDofBd+n5fLLpIIu3ZVNaUQ2Yq8eP7h7ETUO7cHX/CDxcTxv5qK6C/SvNULJrMZSfWusEnwjoe705YhI9UsFEpINTGBGRZlVSXsWSlBw+2XSQdfuP1u4/52mcqnIzmGxfALu+ODOY9JsE/SYrmIh0UAojItJiMvNLWbDlEJ9sOkhG/qnTONGBHiQOjGTS4Ch6h/vUPaiqHPatgB0La0ZMTl1ajHcY9L7GvCIn7hJwdmudDyIiLUphRERa3MlJr59symTxD9mU1JzGAegd5sP1gyO5flAk0YGedQ+sKod9y2H7Qkj9om4wcfWBnuPMYNLzKnD3Q0TapxYLI6tXr+bFF19k06ZNZGdns2DBAiZPnnzOY1auXMnMmTPZvn070dHR/Pa3v2XatGkNfk+FEZG270RFNct25fJZchYrU4/UXo0DkBDjz/WDIrl2YAShPu51D6wqN+8ivGuxeSqnOOfUc3YXc6Skz7XmyInukyPSrrRYGPnyyy9Zu3YtQ4cO5cYbbzxvGElLSyM+Pp4HHniA++67j2XLlvHwww+zePFiJkyY0KwfRkTahoITlfw3JYfPtmbx7b48am6Lg90Go7oHcf2gSMb3CyfA60f3unE4zHvk7FpkBpO81LrPRw4x75HTazyED9I8E5E2rlVO09hstvOGkccee4zFixeTkpJSu+/WW2/l+PHjLFmypEHvozAi0n4dLirjix+y+XRrFlsyjtfud7LbGNUtiKvjwxnfP+zMEROAvD01IyaL4eB3wGl/rrzDoMdVZjDpNlarv4q0QW0mjFx66aUMGTKEl19+uXbfO++8w8MPP0xBQcFZjykvL6e8vLz2cWFhIdHR0QojIu1cZn4pn23NYvEP2XXujWOzwfCugVwdH87V8eFEnr6GyUlFubB7ibkc/b4VUFly6jm7C3QdBT3HQ88JENzTfFERsVRDw4hzvc80k5ycHMLCwursCwsLo7CwkBMnTuDhceYfnaSkJJ5++umWLk1EWll0oCfTx/Zg+tgeHDhawpcpOXyZksPWzONsTM9nY3o+f1i0g0HR/lwTH87E+Ahigmomv/qEwdC7zK2qHA58awaT3f+F/H2QttrcvvotBMSawaT7FRB7Mbj5nLMuEbFWi4+M9OrVi7vvvptZs2bV7vviiy+49tprKS0tPWsY0ciISOeSdfwES1Jy+DIlm+8PHOP0v0p9wn24ql8Y4/qGMSDKD7v9LCMeR/edCiYH1kJ1xann7C7mOibdx5rhJGKQlqcXaSVtZmQkPDyc3NzcOvtyc3Px9fU9axABcHNzw81N6wyIdBaR/h7cc3Ec91wcx+GiMv67PZclKdms35/PrpwiduUU8eryvYT6uHFl31DG9Q1jTI9g3F1qQkVQdwj6BVz0CygvhrRVsHeZefnwsTQ4sMbclj8DHoHQ7XIzmHS/AvyiLP3sItIKYWTUqFF88cUXdfYtXbqUUaNGtfRbi0g7FOrjzp0XdeXOi7pyrKSCFamH+XpnLqtSj3C4qJwPNmbywcZM3F3sXNIzhHF9Q7miTxghPjX/AePmbV4K3Oda83H+fnOOyb7l5mmcE/mwfb65AQT3hm6XQewl5ikdz0BrPrhIJ9bo0zTFxcXs3bsXgISEBGbPns3YsWMJDAwkJiaGWbNmcejQId577z3g1KW906dP55577mH58uU8+OCDurRXRBqlvKqaDfvz+XpnLl/vyCWroKz2OZsNBnXxZ2zvUC7vHVL/6ZzqKji0CfbVjJoc2gSG47QGNgiPh9hLIe5Sc1KsFl0TabIWu5pm5cqVjB079oz9d911F++++y7Tpk0jPT2dlStX1jnm17/+NTt27KBLly48+eSTWvRMRJrMMAx2ZBeybKc5avLDwbpX5gV5uXJprxAu7x3CJT1DCPzxeiYnnTgGad+Yi66lrYYju+o+b7NDZII5ahJ3CcSMAlevFvpUIh2PloMXkU4jt7CM5bsOszL1MGv3HqW4vKr2uZOjJpf3DuHy3qEMiPLD6WyjJmBePpx+Mpx8Y16lczq7C0QNNYNJ3KXQZTi4nH3um4gojIhIJ1VR5WDTgWOs3H2YValH2JVTVOf5QC9XLu4RzMU9gxnTI5ios61pclLBoVOjJmmroSCz7vN2F3PkJOYi6DravGpHc05EaimMiIgAOQVlrNp9mJWpR1izJ4+i00ZNAOKCvRjTI4iLewQzqlswfp4uZ38hw4Bj6adGTdK/gaLsM9uF9D0VTmIuAv+Y5v9QIu2EwoiIyI9UVjvYfOAYa/fmsWZvHlsPFlDtOPUn0G6DAVF+jOlhjpoM7Rpw6vLhHzsZTjLWQ8a3cGAdHN1zZjvfLuZE2JiLIGY0hPTRPXWk01AYERE5j8KySjbsz68NJ3sPF9d53s3ZzvDYQMb0CObiHsH0i/Stf74JQEkeZKwzA8qBbyF7KxjVddu4+5tzTaJHQJdh5hwUXbEjHZTCiIhII+UUlLF2b5657csjt7C8zvN+Hi4Mjw1kZFwgI7sF0i/CF2enc4xyVJSYN/g7GU4OfgeVpT9qZDNHS7oMOxVSgntr9EQ6BIUREZELYBgG+44Us2ZPHmv2HmX9/rpX6QB4uzkzLDaAkXFBjOwWyIAoP1zOFU6qKyHnBzj4vRlMMjfC8QNntnPzhaghZjjpUjOCoomx0g4pjIiINKOqagcpWYVs2H+UDWn5fJeeT1FZ3XDi4eLE0K4BNSMnQQyK9sPN+Tz3wSk+fCqcHPwODm2ue0fikwK7m6d0ooZA5BAIHwCuns34CUWan8KIiEgLqnYY7MwuZENaPhv2H2Vjej7HSyvrtHFztpMQ48+IuCCGdQ1gcIw/vu71XK1T+8JVcGRnzchJTUA528RYmxOE9oXIwWY4iUyAsP7grPt6SduhMCIi0oocDoM9h4vZkHaUDfvz2ZB2lLziijptbDboHebD0K4BtVtMoCc22zkmxQKU5ptL1x/aDFlbIGszFOee2c7J1QwkJ8NJ1BBz/olTi9+GTOSsFEZERCxkzjkpYUPaUTalH+P7A8fIyP/x5FUI9nZjaFf/mnASSHyU7/lP7RiGucbJ6eEka4u5vP2POXuYp3QiBpo/wwdCaD9wcW+mTypSP4UREZE25nBRGZsPHGNTzZZyqJCKakedNq5OdgZ08WNY1wASYgJIiPEnzLcBweHkuicnw8mhLZCdDBXFZ7a1OZlX8NSGlJqg4uHfHB9TpJbCiIhIG1dWWU3KoQI2HTBHTjYfOMbRkooz2kX4uTOoiz+Dov0ZHO3PgC5+eLs14NSLw2HON8n+AXK21vz84ewjKGCuFhs+ECIGnQoovpHm+SWRJlAYERFpZwzD4MDRUr6vGTnZknGM3blFOH70V9pmg56h3gyOPhVQeof5nHvNk1NvAoWHaoLJNjOcZP8ABRlnb+8ZVBNQBkJofwjrB8G9NFFWGkRhRESkAygpryLlUAHJmcfZevA4WzMLOHT8xBnt3F3sxEf61QaUAVF+dA1qwOTYk04cM8PJ6SHlSOqZK8gC2J0hqIc59ySs36mQ4hejxdqkDoUREZEO6nBRGVszC9haE1CSM4+fseYJgI+7M/GRfsRH+RIf5ceAKD9ig7ywn2tJ+9NVnoDDO81gkrMNcnfA4e1QVnD29q7e5uXGof3Mq3pO/tSCbZ2WwoiISCfhcBikHS1ha+bxmhGUAnZmF1JR5TijrY+bM/0ifRkQ5ceALn7ER/kR15iAYhhQmAWHd0BuSk1A2WGOojgqz36Md3jNCEpNOAnpbZ7qcfO5gE8t7YHCiIhIJ1ZZ7WBPbjEphwpIySpg26ECdmQVUn6WgOLl6kT/SDOY9Iv0pW+EDz1DfXB1bsQpl+pKOLoXcrfXBJWaUZTj9cxFAfOOxiG9zSt7QnqZP4N7aSSlA1EYERGROqqqHew9Usy2gwU1IaWQHVmFnKg8c16Ii5ON7iHe9IvwpW+Eb01I8SXQy7Vxb1peZJ7qORlSDu80R1FKDtd/jFfoaSGl96nfvUJ0ZU87ozAiIiLnVe0wbwi47aA5grIzu5Cd2UUUnDj7KZcwXzcznNSElL4RvsQFe+HU0NM8J5XmQ95uOLILjpz8mQqFB+s/xiPAXFH25GmeoB4Q3BP8u2qV2TZKYURERJrEMAyyCsrYmVXIzuxCdmSbP9OPnrmCLJhX8vQO96VfhE9tQOkV6oOf53nuw3M25UU1ISW1blA5lg7U83Vld4aAODOYBHWHoJ6ngopGUyylMCIiIs2quLyK1JxCdmQXmSElq5DUnKKznuYBCPd1p1e4D71CvekV7kPvMB96hnnj6dqEUYzKE+aclJMhJW8PHN1n7qs681LnWm5+ZkAJrgkop2+663GLUxgREZEWV+0wOHC0pHb0ZEdWIbtzi8+6FspJMYGe9ArzpleYD73DfegV5kO3EK/z35PnbBwOcxG3o3tPbXl7zJ/HM6h3NAXMCbQ/DioBceZKtM6NnBsjZ6UwIiIiliksq2RPbjG7c4tIzSliz+EiUnOKySsuP2t7J7uN2CDP2nDSO8yHHqHedA3yatxVPaerLIP8/TUhpWYkJW+P+Xt9S+ID2Ozg1wUCu5lbQFzN73Hm7xpRaTCFERERaXOOFpezuyaknNxSc4ooPMuibQDOdhsxQZ70CPGme6g3PUK86RFq/t6g+/PUpzT/1AjK0T3m7/lpcCwNKs8+N6aWT8RpASW2bmjRzQbrUBgREZF2wTAMcgvL64ST3blF7D1cTEnF2eejgDknpUfoqXDSPcSLHqHehHi7NXwZ/DOLgeJcc0QlP838eazmZ/7++lefPckj0BxBqQ0osebVPgFdzRBjb8KpqHZMYURERNo1wzDIKSxj3+ES9h4uYu+RYvYeLmbfkRKOFJ39dA+Ar7uzGVBqRlHigr3oFuJFdKBn0+alnK40/9QIysmAcjK0nGvtFAC7C/hHnwontT9jzXkqXsEd7sofhREREemwCkor2XukmH2Hi9lXE1L2HikmM7/0jLscn2S3QVSAB3HB3sQFeRIX7EVciDdxQV5EBXg0fq2UHysvMi9BrjOqkg7HD0DBQXCc/VRULRcvM5TUCSqn/XRvf99/CiMiItLplFVWk360xAwnNaMo6XklpOWVUFxefxhwdbITE+RJbJA5ihIX7FX7e6jPBZz2Oam6Coqy4NgBM5wcO2Be7XPy96JsznnlD5iLvvl3NQOLfwz4RZsTbf2jzd89AtrcyIrCiIiISA3DMMgrriAtr4S0vGLS8kprfpaQfrT0rDcVPMnT1YnYIC/iQrzoVhNSugZ5EhPkeWHzU05XVQ7HM+F4+o8CS83PE/nnfw0Xr9PCSRczoPjHnPrdJ6LVV6pVGBEREWmAaodBdsGJmqBSdzt47ATV9Z33wQwqMYGetZsZUrzoGuhJVIAHLk5NvCz5x8qLzJGUkyMqBZk120EzxJxvvgqAzQl8I0+Fk9NHVfyizdNBLh7NU28NhREREZELVFHlIPNYKWlHSkg/WsL+PPO0z4GjpWQXnKh3fgqYc1Qi/T3MgBJYM5pyWmjxcW/Ccvn1qSwzF387nmEGlNqgcvLxQXCc/X5DtRJfgaHTmq8mGv793aTxmtdff50XX3yRnJwcBg0axKuvvsqIESPqbf/yyy8zZ84cMjIyCA4O5qabbiIpKQl3d/emvL2IiEircHW20z3EvDLnxyqqHBw8VsqB/FIy80s5cNTcMvJLyMgvpazSwcFjJzh47ARrOXrG8YFerkQHetK1JpxEB3jSJcCD6EBPwv3cGzeq4uJec1+e7md/3uEwR0+OZ545qnLysV90w9+vmTU6jHz00UfMnDmTN954g5EjR/Lyyy8zYcIEUlNTCQ0NPaP9vHnzePzxx3n77bcZPXo0u3fvZtq0adhsNmbPnt0sH0JERKS1uTrb6RbiTbezBBXDMDhSVM6BmpCScbSEA/mlZOSXknG0lKMlFeTXbFszj59xvN0GEX4eRAV41IYUc/MkOtCDcF93nBsTVux28Ak3t+jhZ29j4YmSRp+mGTlyJMOHD+e1114DwOFwEB0dza9+9Ssef/zxM9rPmDGDnTt3smzZstp9//M//8OGDRtYs2ZNg95Tp2lERKQjKSqrrA0mGfnm6Io5imL+PNeEWjCXz4/wcz8VUE4PLIGehPu6X/ilys2gRU7TVFRUsGnTJmbNmlW7z263M27cONatW3fWY0aPHs3777/Pxo0bGTFiBPv37+eLL77gzjvvrPd9ysvLKS8/taBNYWFhY8oUERFp03zcXegf6Uf/SL8znnM4DPKKy8k8LZyc+nmCQ8dOUFF96hQQnHmljbPdRqS/B1H+HubPAA+i/N2J8vck0t+dSH8P3F3azmqwjQojeXl5VFdXExYWVmd/WFgYu3btOusxt99+O3l5eVx88cUYhkFVVRUPPPAATzzxRL3vk5SUxNNPP92Y0kRERDoEu91GqK87ob7uDO0acMbzDofB4aLy2oCSeXJU5XhpbVipchjmyEt+/ffZCfZ2PRVW/D24fnAkA7v4t+Anq1+LX3C8cuVKnn32Wf76178ycuRI9u7dy0MPPcQzzzzDk08+edZjZs2axcyZM2sfFxYWEh1t3cQaERGRtsJutxHu5064nzvDYs98vtphkFtYRmZ+KVkFZjg5dLyMQ8dPkHXcfHyispq84gryiivYetC8386ALn7tI4wEBwfj5OREbm5unf25ubmEh4ef9Zgnn3ySO++8k/vuuw+AAQMGUFJSws9+9jN+85vfYLefOQHHzc0NNze3xpQmIiIimPNJImtGPM7GMAyOl1Zy6PiJOgGlf6R1czIbFUZcXV0ZOnQoy5YtY/LkyYA5gXXZsmXMmDHjrMeUlpaeETicnMzzVO1giRMREZEOxWazEeDlSoCXK/FRZ85ZsUKjT9PMnDmTu+66i2HDhjFixAhefvllSkpKuPvuuwGYOnUqUVFRJCUlAZCYmMjs2bNJSEioPU3z5JNPkpiYWBtKREREpPNqdBi55ZZbOHLkCE899RQ5OTkMHjyYJUuW1E5qzcjIqDMS8tvf/habzcZvf/tbDh06REhICImJifzpT39qvk8hIiIi7ZaWgxcREZEW0dDv72a6g4+IiIhI0yiMiIiIiKUURkRERMRSCiMiIiJiKYURERERsZTCiIiIiFhKYUREREQspTAiIiIillIYEREREUspjIiIiIilGn1vGiucXLG+sLDQ4kpERESkoU5+b5/vzjPtIowUFRUBEB0dbXElIiIi0lhFRUX4+fnV+3y7uFGew+EgKysLHx8fbDZbs71uYWEh0dHRZGZm6gZ8P6K+OTv1S/3UN2enfqmf+ubsOlK/GIZBUVERkZGR2O31zwxpFyMjdrudLl26tNjr+/r6tvv/wVuK+ubs1C/1U9+cnfqlfuqbs+so/XKuEZGTNIFVRERELKUwIiIiIpbq1GHEzc2N3/3ud7i5uVldSpujvjk79Uv91Ddnp36pn/rm7Dpjv7SLCawiIiLScXXqkRERERGxnsKIiIiIWEphRERERCylMCIiIiKW6tRh5PXXXyc2NhZ3d3dGjhzJxo0brS6pVSUlJTF8+HB8fHwIDQ1l8uTJpKam1mlTVlbG9OnTCQoKwtvbm5/85Cfk5uZaVLE1nnvuOWw2Gw8//HDtvs7cL4cOHeKOO+4gKCgIDw8PBgwYwPfff1/7vGEYPPXUU0RERODh4cG4cePYs2ePhRW3vOrqap588kni4uLw8PCge/fuPPPMM3Xux9FZ+mX16tUkJiYSGRmJzWZj4cKFdZ5vSD/k5+czZcoUfH198ff3595776W4uLgVP0XzO1e/VFZW8thjjzFgwAC8vLyIjIxk6tSpZGVl1XmNjtgvJ3XaMPLRRx8xc+ZMfve737F582YGDRrEhAkTOHz4sNWltZpVq1Yxffp01q9fz9KlS6msrGT8+PGUlJTUtvn1r3/N559/zscff8yqVavIysrixhtvtLDq1vXdd9/xt7/9jYEDB9bZ31n75dixY4wZMwYXFxe+/PJLduzYwZ///GcCAgJq27zwwgv85S9/4Y033mDDhg14eXkxYcIEysrKLKy8ZT3//PPMmTOH1157jZ07d/L888/zwgsv8Oqrr9a26Sz9UlJSwqBBg3j99dfP+nxD+mHKlCls376dpUuXsmjRIlavXs3Pfvaz1voILeJc/VJaWsrmzZt58skn2bx5M/Pnzyc1NZXrr7++TruO2C+1jE5qxIgRxvTp02sfV1dXG5GRkUZSUpKFVVnr8OHDBmCsWrXKMAzDOH78uOHi4mJ8/PHHtW127txpAMa6deusKrPVFBUVGT179jSWLl1qXHbZZcZDDz1kGEbn7pfHHnvMuPjii+t93uFwGOHh4caLL75Yu+/48eOGm5ub8cEHH7RGiZa49tprjXvuuafOvhtvvNGYMmWKYRidt18AY8GCBbWPG9IPO3bsMADju+++q23z5ZdfGjabzTh06FCr1d6SftwvZ7Nx40YDMA4cOGAYRsfvl045MlJRUcGmTZsYN25c7T673c64ceNYt26dhZVZq6CgAIDAwEAANm3aRGVlZZ1+6tOnDzExMZ2in6ZPn861115b5/ND5+6Xzz77jGHDhnHzzTcTGhpKQkICb731Vu3zaWlp5OTk1OkbPz8/Ro4c2aH7ZvTo0Sxbtozdu3cDsHXrVtasWcPEiROBztsvP9aQfli3bh3+/v4MGzasts24ceOw2+1s2LCh1Wu2SkFBATabDX9/f6Dj90u7uFFec8vLy6O6upqwsLA6+8PCwti1a5dFVVnL4XDw8MMPM2bMGOLj4wHIycnB1dW19h/DSWFhYeTk5FhQZev58MMP2bx5M999990Zz3Xmftm/fz9z5sxh5syZPPHEE3z33Xc8+OCDuLq6ctddd9V+/rP92+rIffP4449TWFhInz59cHJyorq6mj/96U9MmTIFoNP2y481pB9ycnIIDQ2t87yzszOBgYGdpq/Kysp47LHHuO2222pvlNfR+6VThhE50/Tp00lJSWHNmjVWl2K5zMxMHnroIZYuXYq7u7vV5bQpDoeDYcOG8eyzzwKQkJBASkoKb7zxBnfddZfF1Vnn3//+N3PnzmXevHn079+f5ORkHn74YSIjIzt1v0jjVVZW8tOf/hTDMJgzZ47V5bSaTnmaJjg4GCcnpzOufsjNzSU8PNyiqqwzY8YMFi1axIoVK+jSpUvt/vDwcCoqKjh+/Hid9h29nzZt2sThw4cZMmQIzs7OODs7s2rVKv7yl7/g7OxMWFhYp+wXgIiICPr161dnX9++fcnIyACo/fyd7d/Wo48+yuOPP86tt97KgAEDuPPOO/n1r39NUlIS0Hn75cca0g/h4eFnXEhQVVVFfn5+h++rk0HkwIEDLF26tHZUBDp+v3TKMOLq6srQoUNZtmxZ7T6Hw8GyZcsYNWqUhZW1LsMwmDFjBgsWLGD58uXExcXVeX7o0KG4uLjU6afU1FQyMjI6dD9deeWVbNu2jeTk5Npt2LBhTJkypfb3ztgvAGPGjDnj8u/du3fTtWtXAOLi4ggPD6/TN4WFhWzYsKFD901paSl2e90/p05OTjgcDqDz9suPNaQfRo0axfHjx9m0aVNtm+XLl+NwOBg5cmSr19xaTgaRPXv28PXXXxMUFFTn+Q7fL1bPoLXKhx9+aLi5uRnvvvuusWPHDuNnP/uZ4e/vb+Tk5FhdWqv5xS9+Yfj5+RkrV640srOza7fS0tLaNg888IARExNjLF++3Pj++++NUaNGGaNGjbKwamucfjWNYXTeftm4caPh7Oxs/OlPfzL27NljzJ071/D09DTef//92jbPPfec4e/vb3z66afGDz/8YEyaNMmIi4szTpw4YWHlLeuuu+4yoqKijEWLFhlpaWnG/PnzjeDgYON///d/a9t0ln4pKioytmzZYmzZssUAjNmzZxtbtmypvSqkIf1w9dVXGwkJCcaGDRuMNWvWGD179jRuu+02qz5SszhXv1RUVBjXX3+90aVLFyM5ObnO3+Py8vLa1+iI/XJSpw0jhmEYr776qhETE2O4uroaI0aMMNavX291Sa0KOOv2zjvv1LY5ceKE8ctf/tIICAgwPD09jRtuuMHIzs62rmiL/DiMdOZ++fzzz434+HjDzc3N6NOnj/Hmm2/Wed7hcBhPPvmkERYWZri5uRlXXnmlkZqaalG1raOwsNB46KGHjJiYGMPd3d3o1q2b8Zvf/KbOF0ln6ZcVK1ac9e/KXXfdZRhGw/rh6NGjxm233WZ4e3sbvr6+xt13320UFRVZ8Gmaz7n6JS0trd6/xytWrKh9jY7YLyfZDOO0JQJFREREWlmnnDMiIiIibYfCiIiIiFhKYUREREQspTAiIiIillIYEREREUspjIiIiIilFEZERETEUgojIiIiYimFEREREbGUwoiIiIhYSmFERERELKUwIiIiIpb6f+fYTcimznb1AAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7426666666666667\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "fashion_mnist = NN(784, 160, 10, 0.03, 130)\n",
        "p = np.random.permutation(len(X))\n",
        "X, y = X[p], y[p]\n",
        "\n",
        "# Splitting the data into training, validation and testing in the ratio 70:20:10\n",
        "X_train, y_train = X[:int(0.7*len(X))], y[:int(0.7*len(X))]\n",
        "X_val, y_val = X[int(0.7*len(X)):int(0.9*len(X))], y[int(0.7*len(X)):int(0.9*len(X))]\n",
        "X_test, y_test = X[int(0.9*len(X)):], y[int(0.9*len(X)):]\n",
        "\n",
        "# Training the model\n",
        "train_loss,val_loss = fashion_mnist.fit(X_train, y_train,X_val,y_val)\n",
        "\n",
        "\n",
        "# Plotting the loss\n",
        "plt.plot(train_loss,label='train')\n",
        "plt.plot(val_loss,label='val')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "y_pred = fashion_mnist.predict(X_test)\n",
        "\n",
        "# [Q18] Why are we using argmax here? Why is this output different from the output of the model?\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "y_test = np.argmax(y_test, axis=1)\n",
        "print(f\"Accuracy: {np.mean(y_pred == y_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-0ns_2m0cgt"
      },
      "outputs": [],
      "source": [
        "print(f\"Accuracy: {np.mean(y_pred == y_test)}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
